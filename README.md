# VICEROY 2026: Hyperdimensional Computing for EW-Resilient Command Classification

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Conference: VICEROY 2026](https://img.shields.io/badge/Conference-VICEROY%202026-green.svg)]()

> **Symposium Poster Companion Repository**  
> "Cognitive Resilience at the Edge: Using Hyperdimensional Computing (HDC) to Secure Autonomous Wingmen (CCA) Against Spectrum Jamming"

---

## ğŸ“‹ Executive Summary

This repository contains the complete simulation code for our VICEROY 2026 symposium poster demonstrating that **Hyperdimensional Computing (HDC)** provides **graceful degradation** (not immunity) against electronic warfare (EW) jamming attacks, outperforming traditional deep learning approaches.

### Key Results at a Glance

| Scenario | Attack Type | HDC Accuracy | MLP Accuracy | Interpretation |
|----------|-------------|--------------|--------------|----------------|
| **A** (ÏƒÂ²=0) | None (clean) | 100% | 98.7% | Both work well |
| **A** (ÏƒÂ²=5) | Russian Broadband | **100%** | 57% | HDC resilient |
| **B** (int=4) | US Precision | 49% | 22% | Both degraded |
| **B** (int=20) | US Precision (extreme) | 22% | 23% | **Both fail** |

---

## ğŸ”¬ Why Does HDC Achieve 100% in Scenario A? (Mathematical Explanation)

We anticipated skepticism about the 100% accuracy under broadband noise. This is **not too good to be true**â€”it's a direct consequence of the mathematics. Here's the rigorous explanation:

### The Setup

- **Input dimension**: n = 50 features
- **Hypervector dimension**: D = 10,000
- **Encoding**: `h = sign(M @ x)` where M âˆˆ â„^(DÃ—n), M[i,j] ~ N(0, 1/âˆšD)
- **Noise model** (Scenario A): x_noisy = x + Îµ, where Îµ ~ N(0, ÏƒÂ²I)

### Why It Works: The Signal-to-Noise Ratio Argument

For a clean input x and noise Îµ, the projection is:

```
M @ x_noisy = M @ x + M @ Îµ
              â†‘         â†‘
           signal     noise
```

**Key insight**: Both signal and noise are projected through the SAME random matrix M.

#### Signal Term: ||M @ x||Â²
- Expected value: E[||M @ x||Â²] = ||x||Â² (variance-normalized projection)
- For our dataset: ||x|| â‰ˆ 3.0 (class centroids scaled by 3.0)

#### Noise Term: ||M @ Îµ||Â²  
- Expected value: E[||M @ Îµ||Â²] = ÏƒÂ² Ã— n (sum of n independent Gaussians)
- At ÏƒÂ² = 5.0: E[||M @ Îµ||Â²] = 5.0 Ã— 50 = 250

**Waitâ€”the noise magnitude is LARGER than the signal!** So why does HDC still work?

### The Sign Function: The Unsung Hero

The critical step is `sign(M @ x_noisy)`. The sign function acts as a **majority vote** across dimensions:

1. Each dimension d of the projection is: `(M @ x)[d] + (M @ Îµ)[d]`

2. The signal component `(M @ x)[d]` has **consistent direction** across all samples of the same class (because they're projected through the same M from similar inputs)

3. The noise component `(M @ Îµ)[d]` is **random and independent** for each sample

4. When we **bundle (sum) many training samples** to form the class prototype:
   - Signal components **add constructively** (all point same direction)
   - Noise components **cancel out** (random directions average to ~0)

### Quantitative Bound

For k training samples per class, the prototype's signal-to-noise ratio improves by âˆšk:

```
SNR_prototype â‰ˆ âˆšk Ã— SNR_single_sample
```

With k = 140 samples per class (700 training / 5 classes):
- âˆš140 â‰ˆ 12Ã— improvement in prototype SNR
- Even if single-sample SNR < 1, prototype SNR >> 1

### Why the MLP Fails

The MLP does NOT benefit from this averaging effect at inference time:
1. Each test sample is classified individually
2. No "prototype averaging" to cancel noise
3. ReLU activations can saturate or explode with noisy inputs
4. Learned weights are optimized for clean data distribution

### The Limit of HDC Robustness

HDC's 100% accuracy in Scenario A is **dataset-dependent**. It works because:
1. Our classes are well-separated (centroid distance >> intra-class variance)
2. The noise variance (ÏƒÂ² = 5) is still within the regime where sign() voting works
3. We have enough training samples for good prototype averaging

**At higher noise levels, HDC would also fail.** The 100% is not magicâ€”it's the sweet spot of our experimental parameters.

---

## ğŸ“Š Honest Assessment: Strengths & Weaknesses

### âœ… Strengths of HDC

| Strength | Evidence | Mechanism |
|----------|----------|-----------|
| **Graceful degradation** | Accuracy drops smoothly, not catastrophically | Distributed representation prevents single points of failure |
| **Noise averaging** | 100% accuracy at ÏƒÂ²=5 (Scenario A) | Random projection + prototype bundling cancels i.i.d. noise |
| **Binary robustness** | sign() clips extreme values | Prevents numerical instability that affects MLPs |
| **Simple training** | No backprop, no hyperparameter tuning | Just matrix multiplication and summation |
| **Interpretable** | Cosine similarity to prototypes | Direct geometric intuition |

### âŒ Weaknesses of HDC

| Weakness | Evidence | Implication |
|----------|----------|-------------|
| **Not immune to extreme noise** | 22% accuracy at intensity=20 (Scenario B) | Fails at ~random guess under concentrated attack |
| **Precision jamming vulnerability** | Drops below 50% at intensity=4 | Targeted attacks are more effective than broadband |
| **High memory footprint** | D=10,000 dimensions per prototype | 5 classes Ã— 10,000 Ã— 4 bytes = 200KB (acceptable but larger than MLP) |
| **Binary quantization loses information** | sign() discards magnitude | May underperform on tasks requiring fine-grained distinctions |
| **Projection matrix must be shared** | Training and inference need same M | Requires secure distribution of the projection matrix |

### âš ï¸ Limitations of This Study

1. **Synthetic dataset**: Real RF signatures may have different statistical properties
2. **i.i.d. noise assumption**: Real jamming may have temporal/spectral structure
3. **No adversarial attacks**: We tested random noise, not optimized adversarial perturbations
4. **Fixed architecture**: We did not tune D, the projection matrix distribution, or encoding schemes
5. **Single random seed**: Results may vary slightly with different random initializations

---

## ğŸ—ï¸ Repository Structure

```
VICEROY_2026_HDC_Sim/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ LICENSE                             # MIT License
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ viceroy_hdc_sim.py                 # V1: Original simulation (bit-flip noise)
â”œâ”€â”€ viceroy_hdc_v2.py                  # V2: Dual-doctrine simulation (RECOMMENDED)
â”œâ”€â”€ viceroy_2026_hdc_results.png       # V1 output visualization
â”œâ”€â”€ viceroy_2026_hdc_results.pdf       # V1 output (print quality)
â”œâ”€â”€ viceroy_2026_v2_dual_doctrine.png  # V2 output visualization
â””â”€â”€ viceroy_2026_v2_dual_doctrine.pdf  # V2 output (print quality)
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- pip package manager

### Installation

```bash
# Clone the repository
git clone https://github.com/CisnerosCodes/VICEROY_2026_HDC_Sim.git
cd VICEROY_2026_HDC_Sim

# Create virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run the Simulation

```bash
# Run V2 (recommended - dual doctrine comparison)
python viceroy_hdc_v2.py

# Run V1 (original bit-flip noise simulation)
python viceroy_hdc_sim.py
```

### Expected Output

The simulation will:
1. Run verification tests to validate HDC mathematical properties
2. Train both HDC and MLP models on clean data
3. Test both models under increasing noise levels
4. Generate publication-quality visualizations (PNG + PDF)
5. Print detailed performance summaries

---

## ğŸ“ Technical Details

### HDC Architecture (V2)

```python
class HDCLearnerV2:
    """
    Random Projection HDC with input normalization.
    
    Encoding: h = sign(M @ normalize(x))
    - M âˆˆ â„^(DÃ—n), M[i,j] ~ N(0, 1/âˆšD)
    - normalize() = StandardScaler (zero mean, unit variance)
    - sign() = bipolar quantization to {-1, +1}
    """
```

### EW Attack Models

| Scenario | Model | Parameters | Real-World Analog |
|----------|-------|------------|-------------------|
| A | Broadband AWGN | ÏƒÂ² âˆˆ [0, 5] on all features | Krasukha-4 area denial |
| B | Precision sweep | 10Ã— intensity on 20% of features, rotating | AN/ALQ-249 surgical jamming |

### Why Random Projection?

The **Johnson-Lindenstrauss Lemma** guarantees that random projection approximately preserves distances:

> For any Îµ > 0 and n points, a random projection into D = O(log(n)/ÎµÂ²) dimensions preserves all pairwise distances within factor (1Â±Îµ).

For our D = 10,000, this provides excellent distance preservation, meaning similar inputs produce similar hypervectors.

---

## ğŸ“ˆ Reproducing Our Results

The simulation uses fixed random seeds for reproducibility:

```python
np.random.seed(2026)  # Main simulation seed
np.random.seed(42)    # Class centroid generation
```

Expected results (may vary Â±2% due to MLP training stochasticity):

**Scenario A (Broadband)**:
- HDC: 100% (ÏƒÂ²=0) â†’ 100% (ÏƒÂ²=5)
- MLP: 98.7% (ÏƒÂ²=0) â†’ 57% (ÏƒÂ²=5)

**Scenario B (Precision)**:
- HDC: 100% (int=0) â†’ 22% (int=20)
- MLP: 98.7% (int=0) â†’ 23% (int=20)

---

## ğŸ“š References

1. Kanerva, P. (2009). "Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors." *Cognitive Computation*.

2. Rahimi, A., et al. (2016). "A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing." *ISLPED*.

3. Johnson, W. B., & Lindenstrauss, J. (1984). "Extensions of Lipschitz mappings into a Hilbert space." *Contemporary Mathematics*.

4. Imani, M., et al. (2019). "A Framework for Collaborative Learning in Secure High-Dimensional Space." *IEEE CLOUD*.

---

## ğŸ¤ Contributing

This is a symposium demonstration project. For questions or collaboration inquiries, please open an issue.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{viceroy2026hdc,
  author = {Cisneros, Adrian},
  title = {VICEROY 2026: HDC for EW-Resilient Command Classification},
  year = {2026},
  publisher = {GitHub},
  url = {https://github.com/CisnerosCodes/VICEROY_2026_HDC_Sim}
}
```

---

## ğŸ“ Contact

**VICEROY 2026 Symposium Poster Session**  
*DoD/Academic Partnership Initiative*

---

*UNCLASSIFIED // FOR OFFICIAL USE ONLY*
